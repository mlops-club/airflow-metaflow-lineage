#!/bin/bash

set -ex

export AWS_PROFILE=sandbox
export AWS_REGION=us-east-1

# aws account id/hash used to make a unique, but consistent bucket name
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --profile ${AWS_PROFILE} --query "Account" --output text)
export AWS_ACCOUNT_ID_HASH=$(echo -n "${AWS_ACCOUNT_ID}" | sha256sum | cut -c5-8)

# hash the acct id and take the first 8 characters
export S3_DATA_LAKE_BUCKET_NAME="airflow-metaflow-${AWS_ACCOUNT_ID_HASH}"
export GLUE_DATABASE="nyc_taxi"

export PULUMI_PROJECT_NAME="airflow-metaflow-lineage"
export PULUMI_STACK_NAME="dev"

# so as not to conflict with one of the datahub services on 8080
# export AIRFLOW__API__PORT=8081
export AIRFLOW__WEBSERVER__WEB_SERVER_PORT=9090
export AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT=9091
export AIRFLOW__LOGGING__TRIGGER_LOG_SERVER_PORT=9092
# export AIRFLOW__CORE__LAZY_LOAD_PLUGINS=False

# export OPENLINEAGE_NAMESPACE="airflow-metaflow"
export OPENLINEAGE_TRANSPORT='{"type": "http", "url": "http://host.docker.internal:8091", "endpoint": "/openapi/openlineage/api/v1/lineage"}'


THIS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"


function airflow() {
    # Managing Airflow Variables using env vars: https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html
    export AIRFLOW_HOME="${THIS_DIR}/airflow"
    export AIRFLOW__CORE__LOAD_EXAMPLES=False
    export AWS_PROFILE=sandbox
    export AWS_REGION=us-east-1
    export AWS_DEFAULT_REGION=us-east-1
    uv run airflow ${@}
}

function airflow-docker() {
    pushd "${THIS_DIR}/airflow"
    docker-compose rm -sf
    docker compose up --build
    popd
}

function start-datahub() {
    DATAHUB_MAPPED_GMS_PORT=8091 uv run datahub docker quickstart
}



function init-airflow() {
    # init airflow sqlite db if not present
    [ -f "${THIS_DIR}/airflow/airflow.db" ] || airflow db reset -y
    # Set Airflow variables
    airflow variables set datalake-aws-region "${AWS_REGION}"
    airflow variables set datalake-s3-bucket "${S3_DATA_LAKE_BUCKET_NAME}"
    airflow variables set datalake-glue-database "${GLUE_DATABASE}"

    airflow connections add  --conn-type 'datahub-rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'

    airflow users create \
        --username test \
        --password test \
        --firstname test \
        --lastname test \
        --role Admin \
        --email test@test.com
}

function start-airflow() {
    airflow standalone
}

function create-infra() {
    export STACK_ACTION=up
    uv run "${THIS_DIR}/infra.py"
}

function destroy-infra() {
    export STACK_ACTION=destroy
    uv run "${THIS_DIR}/infra.py"
}


# remove all files generated by tests, builds, or operating this codebase
function clean {
    rm -rf dist build coverage.xml test-reports sample/ tests/cookiecutter*json
    rm -rf "${THIS_DIR}/airflow/logs/"
    rm -rf "${THIS_DIR}/airflow/airflow.db"
    rm -rf "${THIS_DIR}/airflow/airflow.cfg"
    find . \
      -type d \
      \( \
        -name "*cache*" \
        -o -name "*.dist-info" \
        -o -name "*.egg-info" \
        -o -name "*htmlcov" \
        -o -name "*.metaflow" \
        -o -name "*.metaflow.s3" \
        -o -name "*.mypy_cache" \
        -o -name "*.pytest_cache" \
        -o -name "*.ruff_cache" \
        -o -name "*__pycache__" \
      \) \
      -not -path "*env/*" \
      -exec rm -r {} + || true

    find . \
      -type f \
      -name "*.pyc" \
      -o -name "*.DS_Store" \
      -o -name "*.coverage*" \
      -not -path "*env/*" \
      -exec rm {} +

    airflow db reset -y
}


# print all functions in this file
function help {
    echo "$0 <task> <args>"
    echo "Tasks:"
    compgen -A function | cat -n
}

function generate-metaflow-config() {
    cat > "${THIS_DIR}/metaflow/config.yaml" << EOF
# this file is auto-generaged by 'bash run.sh generate-metaflow-config'
as_of_datetime: "2025-06-01 00:00:00.000"
lookback_days: 30
predict_horizon_hours: 24
glue_database: "${GLUE_DATABASE}"
datalake_s3_bucket: "${S3_DATA_LAKE_BUCKET_NAME}"
region: "${AWS_REGION}"
EOF
    echo "Generated metaflow/config.yaml with:"
    echo "  Glue Database: ${GLUE_DATABASE}"
    echo "  S3 Bucket: ${S3_DATA_LAKE_BUCKET_NAME}"
    echo "  AWS Region: ${AWS_REGION}"
}

function training-flow() {
    generate-metaflow-config
    uv run "./metaflow/train_model_flow.py" --environment=uv ${@}
}

function drop-metaflow-tables() {
    generate-metaflow-config
    echo "Dropping all tables managed by the Metaflow training flow..."
    uv run "./metaflow/drop_tables.py"
}

TIMEFORMAT="Task completed in %3lR"
time ${@:-help}
