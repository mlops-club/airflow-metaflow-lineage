#!/bin/bash

set -e

export AWS_PROFILE=sandbox
export AWS_REGION=us-east-1

# aws account id/hash used to make a unique, but consistent bucket name
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
export AWS_ACCOUNT_ID_HASH=$(echo -n "${AWS_ACCOUNT_ID}" | sha256sum | cut -c5-8)

# hash the acct id and take the first 8 characters
export S3_DATA_LAKE_BUCKET_NAME="airflow-metaflow-${AWS_ACCOUNT_ID_HASH}"
export GLUE_DATABASE="nyc_taxi"

export PULUMI_PROJECT_NAME="airflow-metaflow-lineage"
export PULUMI_STACK_NAME="dev"

THIS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"


function airflow() {
    # Managing Airflow Variables using env vars: https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html
    export AIRFLOW_HOME="${THIS_DIR}/airflow"
    export AIRFLOW__CORE__LOAD_EXAMPLES=False
    export AWS_PROFILE=sandbox
    export AWS_REGION=us-east-1
    export AWS_DEFAULT_REGION=us-east-1
    uvx \
        --with "apache-airflow-providers-amazon" \
        --from "apache-airflow-core>=3.0.2" airflow ${@}
}

function init-airflow() {
    # init airflow sqlite db if not present
    [ -f "${THIS_DIR}/airflow/airflow.db" ] || airflow db reset -y
    # Set Airflow variables
    airflow variables set datalake-aws-region "${AWS_REGION}"
    airflow variables set datalake-s3-bucket "${S3_DATA_LAKE_BUCKET_NAME}"
    airflow variables set datalake-glue-database "${GLUE_DATABASE}"
}

function start-airflow() {
    airflow standalone
}

function create-infra() {
    export STACK_ACTION=up
    uv run "${THIS_DIR}/infra.py"
}

function destroy-infra() {
    export STACK_ACTION=destroy
    uv run "${THIS_DIR}/infra.py"
}


# remove all files generated by tests, builds, or operating this codebase
function clean {
    rm -rf dist build coverage.xml test-reports sample/ tests/cookiecutter*json
    rm -rf "${THIS_DIR}/airflow/logs/"
    rm -rf "${THIS_DIR}/airflow/airflow.db"
    rm -rf "${THIS_DIR}/airflow/airflow.cfg"
    find . \
      -type d \
      \( \
        -name "*cache*" \
        -o -name "*.dist-info" \
        -o -name "*.egg-info" \
        -o -name "*htmlcov" \
        -o -name "*.metaflow" \
        -o -name "*.metaflow.s3" \
        -o -name "*.mypy_cache" \
        -o -name "*.pytest_cache" \
        -o -name "*.ruff_cache" \
        -o -name "*__pycache__" \
      \) \
      -not -path "*env/*" \
      -exec rm -r {} + || true

    find . \
      -type f \
      -name "*.pyc" \
      -o -name "*.DS_Store" \
      -o -name "*.coverage*" \
      -not -path "*env/*" \
      -exec rm {} +

    airflow db reset -y
}


# print all functions in this file
function help {
    echo "$0 <task> <args>"
    echo "Tasks:"
    compgen -A function | cat -n
}

function generate-metaflow-config() {
    cat > "${THIS_DIR}/metaflow/config.yaml" << EOF
as_of_datetime: "2025-06-01 00:00:00.000"
lookback_days: 30
predict_horizon_hours: 24
glue_database: "${GLUE_DATABASE}"
s3_bucket: "${S3_DATA_LAKE_BUCKET_NAME}"
region: "${AWS_REGION}"
EOF
    echo "Generated metaflow/config.yaml with:"
    echo "  Glue Database: ${GLUE_DATABASE}"
    echo "  S3 Bucket: ${S3_DATA_LAKE_BUCKET_NAME}"
    echo "  AWS Region: ${AWS_REGION}"
}

function run-metaflow-training() {
    generate-metaflow-config
    uv run "./metaflow/train_model_flow.py" --environment=local ${@}
}

TIMEFORMAT="Task completed in %3lR"
time ${@:-help}
