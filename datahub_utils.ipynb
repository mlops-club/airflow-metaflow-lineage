{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f16a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import datahub.emitter.mce_builder as builder\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def get_athena_table_dataset_urn(catalog: str, database: str, table: str, region: str) -> str:\n",
    "    \"\"\"\n",
    "    e.g. urn:li:dataset:(urn:li:dataPlatform:hive,/iceberg/yellow_rides_hourly_actuals,PROD)\n",
    "    \"\"\"\n",
    "    athena_client = boto3.client(\"athena\", region_name=region)\n",
    "    table_metadata = athena_client.get_table_metadata(CatalogName=catalog, DatabaseName=database, TableName=table)\n",
    "\n",
    "    # Dataset has also its' physical location which we can add in symlink facet.\n",
    "    s3_location = table_metadata[\"TableMetadata\"][\"Parameters\"][\"location\"]\n",
    "    parsed_path = urlparse(s3_location)\n",
    "\n",
    "    return builder.make_dataset_urn(\n",
    "        platform=\"hive\",\n",
    "        name=parsed_path.path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1927d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    get_athena_table_dataset_urn(\n",
    "        catalog=\"AwsDataCatalog\",\n",
    "        database=\"nyc_taxi\",\n",
    "        table=\"yellow_rides_hourly_actuals\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "    == \"urn:li:dataset:(urn:li:dataPlatform:hive,/iceberg/yellow_rides_hourly_actuals,PROD)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8bf9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahub.emitter.serialization_helper import pre_json_transform\n",
    "\n",
    "\n",
    "def make_assertion_urn(dataset_urn: str, assertion_name: str) -> str:\n",
    "    return builder.make_assertion_urn(\n",
    "        builder.datahub_guid(\n",
    "            pre_json_transform(\n",
    "                # these key-val pairs are essentially hashed; we want to choose pairs\n",
    "                # that make the assertions unique (example: https://github.com/datahub-project/datahub/blob/d2d9d36987f20a9f7d6c973073d1404edf33e667/metadata-ingestion-modules/gx-plugin/src/datahub_gx_plugin/action.py#L277-L289)\n",
    "                {\n",
    "                    \"platform\": \"pattern-ds-dqv\",\n",
    "                    # bad name since assertions and datasets have a many-to-many relationship\n",
    "                    \"dataset_urn\": dataset_urn,\n",
    "                    \"assertion_name\": assertion_name,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62baab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urn:li:assertion:eb902459c44c5c939f980eee43d7b73c'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_rides_hourly_actuals__dataset_urn: str = get_athena_table_dataset_urn(\n",
    "    catalog=\"AwsDataCatalog\",\n",
    "    database=\"nyc_taxi\",\n",
    "    table=\"yellow_rides_hourly_actuals\",\n",
    "    region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "dummy_assertion_arn: str = make_assertion_urn(\n",
    "    dataset_urn=yellow_rides_hourly_actuals__dataset_urn,\n",
    "    assertion_name=\"test_assertion\",\n",
    ")\n",
    "\n",
    "dummy_assertion_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a364f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from datahub.ingestion.graph.config import ClientMode\n",
    "\n",
    "emitter = DatahubRestEmitter(\n",
    "    gms_server=\"http://localhost:8091\",\n",
    "    # token=self.token,\n",
    "    # read_timeout_sec=self.timeout_sec,\n",
    "    # connect_timeout_sec=self.timeout_sec,\n",
    "    # retry_status_codes=self.retry_status_codes,\n",
    "    # retry_max_times=self.retry_max_times,\n",
    "    # extra_headers=self.extra_headers,\n",
    "    client_mode=ClientMode.INGESTION,\n",
    "    datahub_component=\"gx-plugin\",\n",
    ")\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from datahub.metadata.com.linkedin.pegasus2avro.assertion import (\n",
    "    AssertionResult,\n",
    "    AssertionResultType,\n",
    "    AssertionRunEvent,\n",
    "    AssertionRunStatus,\n",
    ")\n",
    "\n",
    "assertion_result = AssertionRunEvent(\n",
    "    timestampMillis=int(round(time.time() * 1000)),\n",
    "    assertionUrn=dummy_assertion_arn,\n",
    "    asserteeUrn=yellow_rides_hourly_actuals__dataset_urn,\n",
    "    runId=datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    result=AssertionResult(\n",
    "        type=AssertionResultType.FAILURE,  # if success else AssertionResultType.FAILURE),\n",
    "        rowCount=1000,  # presumably, the number of rows that were tested\n",
    "        missingCount=5,  # presumably, the number of rows that were NULL,\n",
    "        unexpectedCount=10,  # presumably, the number of rows that failed the test\n",
    "        # actualAggValue=actualAggValue,  # use this if the assertion COUNTs, MAXs, MINs, things. E.g. if you are counting the number of unique values for a check, or asserring that the STDEV is within a certain bound\n",
    "        #      For expect_column_mean_to_be_between(min=10, max=20) â†’ actualAggValue might be 15.7\n",
    "        externalUrl=\"https://ericriddoch.info\",  # a placeholder website, this could potentially be the Metaflow HTML that DS DQV generates!,\n",
    "        # nativeResults=nativeResults,\n",
    "    ),\n",
    "    # Batch spec is for when an assertion is run on a SUBSET of a porentially LARGE\n",
    "    # dataset. E.g. if you get new data 1x/day... don't re-test your entire historical\n",
    "    # dataset. Just test the new data. Here's an example\n",
    "    #\n",
    "    # batchSpec = BatchSpec(\n",
    "    #    nativeBatchId=\"unique-batch-identifier\",\n",
    "    #    query=\"SELECT * FROM table WHERE date = '2024-01-01'\",  # SQL query if applicable\n",
    "    #    limit=1000,  # Optional row limit\n",
    "    #    customProperties={\n",
    "    #        \"data_asset_name\": \"my_table\",\n",
    "    #        \"datasource_name\": \"my_database\",\n",
    "    #        \"batch_timestamp\": \"2024-01-01T00:00:00Z\"\n",
    "    #    }\n",
    "    # . )\n",
    "    # batchSpec=ds[\"batchSpec\"],\n",
    "    status=AssertionRunStatus.COMPLETE,\n",
    "    runtimeContext={\n",
    "        \"some\": \"arbitrary params\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a070e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahub.emitter.mcp import MetadataChangeProposalWrapper\n",
    "\n",
    "dataset_assertionResult_mcp = MetadataChangeProposalWrapper(\n",
    "    entityUrn=assertion_result.assertionUrn,\n",
    "    aspect=assertion_result,\n",
    ")\n",
    "\n",
    "emitter.emit_mcp(dataset_assertionResult_mcp)  # , emit_mode=\"SYNC\") TODO where does thie enum import from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b5f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow-metaflow-lineage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
