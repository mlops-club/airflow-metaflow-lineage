{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada84b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import datahub.emitter.mce_builder as builder\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def get_athena_table_dataset_urn(catalog: str, database: str, table: str, region: str) -> str:\n",
    "    \"\"\"\n",
    "    e.g. urn:li:dataset:(urn:li:dataPlatform:hive,/iceberg/yellow_rides_hourly_actuals,PROD)\n",
    "    \"\"\"\n",
    "    session = boto3.Session(profile_name=\"sandbox\")\n",
    "    athena_client = session.client(\"athena\", region_name=region)\n",
    "    table_metadata = athena_client.get_table_metadata(CatalogName=catalog, DatabaseName=database, TableName=table)\n",
    "\n",
    "    # Dataset has also its' physical location which we can add in symlink facet.\n",
    "    s3_location = table_metadata[\"TableMetadata\"][\"Parameters\"][\"location\"]\n",
    "    parsed_path = urlparse(s3_location)\n",
    "\n",
    "    return builder.make_dataset_urn(\n",
    "        platform=\"hive\",\n",
    "        name=parsed_path.path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c92db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahub.emitter.serialization_helper import pre_json_transform\n",
    "\n",
    "\n",
    "def make_assertion_urn(dataset_urn: str, assertion_name: str) -> str:\n",
    "    return builder.make_assertion_urn(\n",
    "        builder.datahub_guid(\n",
    "            pre_json_transform(\n",
    "                # these key-val pairs are essentially hashed; we want to choose pairs\n",
    "                # that make the assertions unique (example: https://github.com/datahub-project/datahub/blob/d2d9d36987f20a9f7d6c973073d1404edf33e667/metadata-ingestion-modules/gx-plugin/src/datahub_gx_plugin/action.py#L277-L289)\n",
    "                {\n",
    "                    \"platform\": \"pattern-ds-dqv\",\n",
    "                    # bad name since assertions and datasets have a many-to-many relationship\n",
    "                    \"dataset_urn\": dataset_urn,\n",
    "                    \"assertion_name\": assertion_name,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed5d462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_assertion_result_shared',\n",
       " '_run_assertion_result_shared',\n",
       " '_run_assertion_build_params',\n",
       " 'run_assertion',\n",
       " 'run_assertions',\n",
       " 'run_assertions_for_asset',\n",
       " 'upsert_custom_assertion',\n",
       " 'report_assertion_result']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inlined from /metadata-ingestion/examples/library/upsert_custom_assertion.py\n",
    "\n",
    "from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph\n",
    "\n",
    "graph = DataHubGraph(config=DatahubClientConfig(server=\"http://localhost:8091\"))\n",
    "\n",
    "[methods for methods in graph.__dir__() if \"assertion\" in methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8529b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_urn = get_athena_table_dataset_urn(\n",
    "    catalog=\"AwsDataCatalog\",\n",
    "    database=\"nyc_taxi\",\n",
    "    table=\"yellow_rides_hourly_actuals\",\n",
    "    region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "assertion_urn = make_assertion_urn(\n",
    "    dataset_urn=entity_urn,\n",
    "    assertion_name=\"test_assertion\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817674b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'urn': 'urn:li:assertion:eb902459c44c5c939f980eee43d7b73c'}\n"
     ]
    }
   ],
   "source": [
    "# Upsert the assertion\n",
    "res = graph.upsert_custom_assertion(\n",
    "    urn=assertion_urn,  # If the assertion already exists, provide the URN\n",
    "    entity_urn=entity_urn,\n",
    "    type=\"DQV\",  # This categorizes your assertion in DataHub\n",
    "    description=\"The description of my external assertion for my dataset\",\n",
    "    # platform_urn=\"urn:li:dataPlatform:great-expectations\", # OR you can provide 'platformName=\"My Custom Platform\"'\n",
    "    platform_name=\"metaflow\",\n",
    "    field_path=\"field_foo\",  # Optional: if you want to associate it with a specific field\n",
    "    # external_url=\"https://my-monitoring-tool.com/result-for-this-assertion\",  # Optional: link to monitoring tool\n",
    "    # logic=\"SELECT * FROM X WHERE Y\",  # Optional: custom SQL for the assertion, rendered in the UI\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54fc136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully reported Assertion Result!\n"
     ]
    }
   ],
   "source": [
    "res = graph.report_assertion_result(\n",
    "    urn=assertion_urn,  # Replace with your actual assertion URN\n",
    "    timestamp_millis=int(time.time() * 1000),  # Current Unix timestamp in milliseconds\n",
    "    type=\"SUCCESS\",  # Can be 'SUCCESS', 'FAILURE', 'ERROR', or 'INIT'\n",
    "    properties=[\n",
    "        {\"key\": \"expected value\", \"value\": \"less than 20\"},  # Example property, can be any key-value pair\n",
    "        {\"key\": \"actual value\", \"value\": \"10\"},\n",
    "    ],\n",
    "    # external_url=\"https://my-great-expectations.com/results/1234\",  # Optional: URL to the results in the external tool\n",
    "    # Uncomment the following section and use if type is 'ERROR'\n",
    "    # error_type=\"UNKNOWN_ERROR\",  # Can be 'VALIDATION_ERROR', 'SYSTEM_ERROR', or 'OTHER' ## ENUM\n",
    "    # error_message=\"<ERROR MESSAGE>\",  ## does not show in UI\n",
    ")\n",
    "\n",
    "print(\"Successfully reported Assertion Result!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9518903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_dqv_tool import dqv_check, recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f71850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.DataFrame({\"foo1\": [1, 2, 3], \"foo2\": [4, 5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b397e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqv_results = dqv_check(\n",
    "    dataset_type=\"pandas\",\n",
    "    dataset_name=\"foo\",\n",
    "    dataset=foo,\n",
    "    checks={\n",
    "        \"foo1\": {\n",
    "            \"missing_percent\": [[\"eq\", 0, \"fail\"]],\n",
    "            \"min\": [[\"gt\", 0, \"fail\"]],\n",
    "            \"max\": [[\"lt\", 10, \"fail\"]],\n",
    "            \"mean\": [[\"gt\", 4, \"fail\"]],\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a62f876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': [{'dataset_name': 'foo',\n",
       "   'dataset_owner': {},\n",
       "   'dataset_type': 'pandas',\n",
       "   'checks': {'foo1': {'missing_percent': [{'condition': 'eq',\n",
       "       'value': 0,\n",
       "       'criticality': 'fail',\n",
       "       'calculated_value': np.float64(0.0)}],\n",
       "     'min': [{'condition': 'gt',\n",
       "       'value': 0,\n",
       "       'criticality': 'fail',\n",
       "       'calculated_value': np.int64(1)}],\n",
       "     'max': [{'condition': 'lt',\n",
       "       'value': 10,\n",
       "       'criticality': 'fail',\n",
       "       'calculated_value': np.int64(3)}]}}}],\n",
       " 'warning': [],\n",
       " 'failed': [{'dataset_name': 'foo',\n",
       "   'dataset_owner': {},\n",
       "   'dataset_type': 'pandas',\n",
       "   'checks': {'foo1': {'mean': [{'condition': 'gt',\n",
       "       'value': 4,\n",
       "       'criticality': 'fail',\n",
       "       'calculated_value': np.float64(2.0)}]}}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datahub_report_update(\n",
    "    entity_urn,\n",
    "    assertion_urn,\n",
    "    status,\n",
    "    checks_description,\n",
    "    properties,\n",
    "):\n",
    "    graph.upsert_custom_assertion(\n",
    "        urn=assertion_urn,\n",
    "        entity_urn=entity_urn,\n",
    "        type=\"DQV\",  # This categorizes your assertion in DataHub\n",
    "        description=checks_description,\n",
    "        # platform_urn=\"urn:li:dataPlatform:great-expectations\", # OR you can provide 'platformName=\"My Custom Platform\"'\n",
    "        platform_name=\"metaflow\",\n",
    "        # external_url=\"https://my-monitoring-tool.com/result-for-this-assertion\",  # Optional: link to monitoring tool\n",
    "    )\n",
    "\n",
    "    graph.report_assertion_result(\n",
    "        urn=assertion_urn,\n",
    "        timestamp_millis=int(time.time() * 1000),\n",
    "        type=status,\n",
    "        properties=properties,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19ea16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_to_urn = {\n",
    "    \"foo\": entity_urn,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "874d7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_dqv_tool.recipes import condition_description_map, metric_description_map\n",
    "\n",
    "\n",
    "def log_dqv_report_datahub(dqv_results, dataset_name_to_urn):\n",
    "    for status in [\"passed\", \"failed\"]:\n",
    "        for result in dqv_results[status]:\n",
    "            for column, metrics in result[\"checks\"].items():\n",
    "                for metric, conditions in metrics.items():\n",
    "                    metric_desc = metric_description_map.get(metric, metric)\n",
    "                    for condition_tuple in conditions:\n",
    "                        condition = condition_tuple[\"condition\"]\n",
    "                        value = condition_tuple[\"value\"]\n",
    "                        actual = condition_tuple[\"calculated_value\"]\n",
    "                        cond_desc = condition_description_map.get(condition, condition)\n",
    "                        properties = [\n",
    "                            {\"key\": \"column\", \"value\": column},\n",
    "                            {\"key\": \"metric\", \"value\": metric},\n",
    "                            {\"key\": \"condition\", \"value\": condition},\n",
    "                            {\"key\": \"expected\", \"value\": value},\n",
    "                            {\"key\": \"actual\", \"value\": float(actual)},\n",
    "                        ]\n",
    "                        datahub_report_update(\n",
    "                            entity_urn=dataset_name_to_urn[result[\"dataset_name\"]],\n",
    "                            assertion_urn=make_assertion_urn(\n",
    "                                dataset_urn=dataset_name_to_urn[result[\"dataset_name\"]],\n",
    "                                assertion_name=f\"{column}_{metric}_{condition}_{value}\",\n",
    "                            ),\n",
    "                            status=\"SUCCESS\" if status == \"passed\" else \"FAILURE\",\n",
    "                            checks_description=f\"Column: {column} - {metric_desc} value {cond_desc} {value}\",\n",
    "                            properties=properties,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498da204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dqv_report_datahub(dqv_results, dataset_name_to_urn={\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ba793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f7037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b872e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow-metaflow-lineage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
